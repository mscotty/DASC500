{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76450038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:04:48,679 - INFO - Starting Part 1A: Outlier Detection an.d Reporting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:04:48,681 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:65 - DatabaseManager initialized for DB: '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'\n",
      "2025-06-02 04:04:48,686 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:101 - Connecting to database: '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'...\n",
      "2025-06-02 04:04:48,690 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:105 - Database connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:04:48,692 - INFO - Attempting to load table 'military-bases' from '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:04:49,009 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:259 - SELECT query executed successfully. Found 776 records.\n",
      "2025-06-02 04:04:49,021 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:118 - Committing final changes (if any) and closing connection...\n",
      "2025-06-02 04:04:49,022 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:122 - Database connection to '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db' closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:04:49,023 - INFO - Successfully loaded 776 rows from table 'military-bases'.\n",
      "2025-06-02 04:04:49,025 - INFO - DataFrame columns: ['Geo Point', 'Geo Shape', 'OBJECTID_1', 'OBJECTID', 'COMPONENT', 'Site Name', 'Joint Base', 'State Terr', 'COUNTRY', 'Oper Stat', 'PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
      "2025-06-02 04:04:49,031 - INFO - Proceeding with columns for analysis: ['PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
      "2025-06-02 04:04:49,032 - INFO - Running comprehensive outlier detection...\n",
      "\n",
      "================================================================================\n",
      "OUTLIER DETECTION REPORT - 20250602_040449\n",
      "================================================================================\n",
      "DataFrame shape: (776, 14) (rows, columns)\n",
      "Memory usage: 27.31 MB\n",
      "Replotting enabled with method: consensus\n",
      "Minimum consensus methods: 2\n",
      "================================================================================\n",
      "Analyzing 4 numerical columns for outliers\n",
      "Methods: iqr, zscore, pca\n",
      "================================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "DESCRIPTIVE STATISTICS ANALYSIS\n",
      "==================================================\n",
      "\n",
      "=== Enhanced Descriptive Statistics Analysis ===\n",
      "Analyzing 4 numerical columns\n",
      "\n",
      "Descriptive Statistics:\n",
      "            count          mean           std          min            25%  \\\n",
      "PERIMETER   776.0  2.069916e+01  4.384776e+01     0.209219       2.235178   \n",
      "AREA        776.0  4.665591e+01  2.579263e+02     0.002528       0.186306   \n",
      "Shape_Leng  776.0  4.343185e+04  9.281859e+04   340.053893    4620.498112   \n",
      "Shape_Area  776.0  2.008972e+08  1.128022e+09  6966.990351  734091.701806   \n",
      "\n",
      "                     50%           75%           max   skewness    kurtosis  \\\n",
      "PERIMETER   6.321396e+00  1.847739e+01  4.890933e+02   4.985964   32.981316   \n",
      "AREA        1.037202e+00  7.776004e+00  4.513484e+03  11.523200  166.106677   \n",
      "Shape_Leng  1.288014e+04  3.958098e+04  1.017174e+06   4.999538   32.811533   \n",
      "Shape_Area  4.348185e+06  3.147674e+07  1.851155e+10  11.091535  147.203214   \n",
      "\n",
      "                  cv            1%             5%           95%           99%  \\\n",
      "PERIMETER   2.118335      0.462582       0.867731  8.964733e+01  2.278280e+02   \n",
      "AREA        5.528267      0.010601       0.030866  1.558152e+02  1.107019e+03   \n",
      "Shape_Leng  2.137109    962.983130    1697.315028  1.939205e+05  4.569172e+05   \n",
      "Shape_Area  5.614922  43540.601018  123309.498358  6.047682e+08  4.560999e+09   \n",
      "\n",
      "                   range           iqr  missing_count  missing_percent  \n",
      "PERIMETER   4.888841e+02  1.624221e+01              0              0.0  \n",
      "AREA        4.513481e+03  7.589697e+00              0              0.0  \n",
      "Shape_Leng  1.016834e+06  3.496048e+04              0              0.0  \n",
      "Shape_Area  1.851154e+10  3.074265e+07              0              0.0  \n",
      "Statistics saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/descriptive_stats/descriptive_statistics.csv\n",
      "\n",
      "Normality Test Results:\n",
      "  PERIMETER: p-value = 0.0000 - Not normal\n",
      "  AREA: p-value = 0.0000 - Not normal\n",
      "  Shape_Leng: p-value = 0.0000 - Not normal\n",
      "  Shape_Area: p-value = 0.0000 - Not normal\n",
      "Normality tests saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/descriptive_stats/normality_tests.csv\n",
      "\n",
      "Creating boxplots...\n",
      "\n",
      "Creating histograms...\n",
      "\n",
      "Creating violin plots...\n",
      "\n",
      "=== Descriptive Statistics Analysis Complete ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "Z-SCORE OUTLIER DETECTION\n",
      "==================================================\n",
      "\n",
      "=== Z-Score Outlier Detection ===\n",
      "Analyzing 4 columns for outliers using z-score threshold: 3.0\n",
      "Detected 22 outliers (2.84% of data)\n",
      "  - PERIMETER: 21 outliers\n",
      "  - AREA: 13 outliers\n",
      "  - Shape_Leng: 19 outliers\n",
      "  - Shape_Area: 12 outliers\n",
      "Outliers saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/zscore/zscore_outliers.csv\n",
      "\n",
      "Creating z-score distribution plots...\n",
      "Z-score distribution plots saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/zscore\n",
      "\n",
      "=== Z-Score Outlier Detection Complete ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "IQR OUTLIER DETECTION\n",
      "==================================================\n",
      "\n",
      "=== IQR-Based Outlier Detection ===\n",
      "Analyzing 4 columns for outliers using IQR method (k=1.5)\n",
      "\n",
      "Analyzing column: PERIMETER\n",
      "  Found 86 outliers (11.08% of data)\n",
      "  IQR: 16.2422\n",
      "  Bounds: [-22.1281, 42.8407]\n",
      "\n",
      "Analyzing column: AREA\n",
      "  Found 129 outliers (16.62% of data)\n",
      "  IQR: 7.5897\n",
      "  Bounds: [-11.1982, 19.1606]\n",
      "\n",
      "Analyzing column: Shape_Leng\n",
      "  Found 87 outliers (11.21% of data)\n",
      "  IQR: 34960.4850\n",
      "  Bounds: [-47820.2294, 92021.7106]\n",
      "\n",
      "Analyzing column: Shape_Area\n",
      "  Found 126 outliers (16.24% of data)\n",
      "  IQR: 30742651.0688\n",
      "  Bounds: [-45379884.9014, 77590719.3738]\n",
      "\n",
      "Total outliers detected: 428\n",
      "\n",
      "Outliers by column:\n",
      "  AREA: 129 outliers\n",
      "  Shape_Area: 126 outliers\n",
      "  Shape_Leng: 87 outliers\n",
      "  PERIMETER: 86 outliers\n",
      "\n",
      "Outliers saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/iqr/iqr_outliers.csv\n",
      "\n",
      "Total rows flagged as containing outliers: 142 (18.30% of data)\n",
      "Flagged data saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/iqr/flagged_data_iqr.csv\n",
      "\n",
      "Creating IQR boxplots...\n",
      "\n",
      "Creating IQR histograms...\n",
      "\n",
      "=== IQR-Based Outlier Detection Complete ===\n",
      "\n",
      "\n",
      "==================================================\n",
      "PCA-BASED OUTLIER ANALYSIS\n",
      "==================================================\n",
      "\n",
      "=== Principal Component Analysis (PCA) ===\n",
      "Performing PCA on 4 variables with 776 samples\n",
      "Variables: ['PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
      "Optimal number of components for 95.0% explained variance: 2\n",
      "Kaiser Criterion recommended number of components: 1\n",
      "\n",
      "Number of components selected: 2\n",
      "Total explained variance: 0.9827\n",
      "\n",
      "Explained variance by component:\n",
      "  PC1: 0.8836 (88.36%)\n",
      "  PC2: 0.0990 (9.90%)\n",
      "\n",
      "Top loadings by component:\n",
      "  PC1:\n",
      "    Shape_Leng: 0.5048\n",
      "    PERIMETER: 0.5024\n",
      "    AREA: 0.4971\n",
      "  PC2:\n",
      "    Shape_Area: 0.5118\n",
      "    PERIMETER: -0.5081\n",
      "    AREA: 0.4952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scree plot saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/pca/pca_scree_plot.png\n",
      "PCA scatter plot saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/pca/pca_scatter_plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PCA Analysis Complete ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA loadings heatmap saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/pca/pca_loadings_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "COMBINED OUTLIER ANALYSIS\n",
      "==================================================\n",
      "\n",
      "Outliers by method:\n",
      "  zscore: 22 outliers\n",
      "  iqr: 142 outliers\n",
      "  pca: 83 outliers\n",
      "\n",
      "Method agreement:\n",
      "  3 methods: 22 outliers\n",
      "  2 methods: 61 outliers\n",
      "  1 methods: 59 outliers\n",
      "\n",
      "Consensus outliers saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/consensus_outliers.csv\n",
      "\n",
      "\n",
      "==================================================\n",
      "ANALYSIS WITHOUT OUTLIERS\n",
      "==================================================\n",
      "Consensus method: found 83 outliers agreed upon by ≥2 methods\n",
      "Removed 83 outlier rows using 'consensus' method\n",
      "Original data shape: (776, 14)\n",
      "Cleaned data shape: (693, 14)\n",
      "Reduction: 83 rows (10.70%)\n",
      "\n",
      "Running descriptive statistics on cleaned data...\n",
      "\n",
      "=== Enhanced Descriptive Statistics Analysis ===\n",
      "Analyzing 4 numerical columns\n",
      "\n",
      "Descriptive Statistics:\n",
      "            count          mean           std          min            25%  \\\n",
      "PERIMETER   693.0  9.131361e+00  9.740092e+00     0.209219       2.041149   \n",
      "AREA        693.0  4.968504e+00  1.095002e+01     0.002528       0.160753   \n",
      "Shape_Leng  693.0  1.902550e+04  2.064837e+04   340.053893    4220.150127   \n",
      "Shape_Area  693.0  2.152587e+07  5.060754e+07  6966.990351  664160.380745   \n",
      "\n",
      "                     50%           75%           max  skewness   kurtosis  \\\n",
      "PERIMETER   4.963666e+00  1.330399e+01  4.419571e+01  1.540173   1.794461   \n",
      "AREA        6.656519e-01  4.531683e+00  8.534156e+01  3.885353  17.917786   \n",
      "Shape_Leng  1.031224e+04  2.696312e+04  1.017347e+05  1.617311   2.130539   \n",
      "Shape_Area  2.727363e+06  1.738573e+07  4.938087e+08  4.385446  24.499278   \n",
      "\n",
      "                  cv            1%             5%           95%           99%  \\\n",
      "PERIMETER   1.066664      0.411604       0.835381  3.007357e+01  4.128958e+01   \n",
      "AREA        2.203887      0.010060       0.026408  2.753765e+01  5.509839e+01   \n",
      "Shape_Leng  1.085300    904.878070    1642.888780  6.589838e+04  8.739593e+04   \n",
      "Shape_Area  2.351010  41866.562169  109793.233836  1.237853e+08  2.471477e+08   \n",
      "\n",
      "                   range           iqr  missing_count  missing_percent  \n",
      "PERIMETER   4.398649e+01  1.126284e+01              0              0.0  \n",
      "AREA        8.533903e+01  4.370930e+00              0              0.0  \n",
      "Shape_Leng  1.013947e+05  2.274296e+04              0              0.0  \n",
      "Shape_Area  4.938018e+08  1.672157e+07              0              0.0  \n",
      "Statistics saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/descriptive_stats/descriptive_statistics.csv\n",
      "\n",
      "Normality Test Results:\n",
      "  PERIMETER: p-value = 0.0000 - Not normal\n",
      "  AREA: p-value = 0.0000 - Not normal\n",
      "  Shape_Leng: p-value = 0.0000 - Not normal\n",
      "  Shape_Area: p-value = 0.0000 - Not normal\n",
      "Normality tests saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/descriptive_stats/normality_tests.csv\n",
      "\n",
      "Creating boxplots...\n",
      "\n",
      "Creating histograms...\n",
      "\n",
      "Creating violin plots...\n",
      "\n",
      "=== Descriptive Statistics Analysis Complete ===\n",
      "\n",
      "Running outlier detection on cleaned data for comparison...\n",
      "  Z-score analysis on cleaned data...\n",
      "\n",
      "=== Z-Score Outlier Detection ===\n",
      "Analyzing 4 columns for outliers using z-score threshold: 3.0\n",
      "Detected 29 outliers (4.18% of data)\n",
      "  - PERIMETER: 12 outliers\n",
      "  - AREA: 17 outliers\n",
      "  - Shape_Leng: 13 outliers\n",
      "  - Shape_Area: 19 outliers\n",
      "Outliers saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/zscore/zscore_outliers.csv\n",
      "\n",
      "Creating z-score distribution plots...\n",
      "Z-score distribution plots saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/zscore\n",
      "\n",
      "=== Z-Score Outlier Detection Complete ===\n",
      "  IQR analysis on cleaned data...\n",
      "\n",
      "=== IQR-Based Outlier Detection ===\n",
      "Analyzing 4 columns for outliers using IQR method (k=1.5)\n",
      "\n",
      "Analyzing column: PERIMETER\n",
      "  Found 35 outliers (5.05% of data)\n",
      "  IQR: 11.2628\n",
      "  Bounds: [-14.8531, 30.1983]\n",
      "\n",
      "Analyzing column: AREA\n",
      "  Found 80 outliers (11.54% of data)\n",
      "  IQR: 4.3709\n",
      "  Bounds: [-6.3956, 11.0881]\n",
      "\n",
      "Analyzing column: Shape_Leng\n",
      "  Found 43 outliers (6.20% of data)\n",
      "  IQR: 22742.9650\n",
      "  Bounds: [-29894.2973, 61077.5625]\n",
      "\n",
      "Analyzing column: Shape_Area\n",
      "  Found 86 outliers (12.41% of data)\n",
      "  IQR: 16721570.5003\n",
      "  Bounds: [-24418195.3696, 42468086.6314]\n",
      "\n",
      "Total outliers detected: 244\n",
      "\n",
      "Outliers by column:\n",
      "  Shape_Area: 86 outliers\n",
      "  AREA: 80 outliers\n",
      "  Shape_Leng: 43 outliers\n",
      "  PERIMETER: 35 outliers\n",
      "\n",
      "Outliers saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/iqr/iqr_outliers.csv\n",
      "\n",
      "Total rows flagged as containing outliers: 97 (14.00% of data)\n",
      "Flagged data saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/iqr/flagged_data_iqr.csv\n",
      "\n",
      "Creating IQR boxplots...\n",
      "\n",
      "Creating IQR histograms...\n",
      "\n",
      "=== IQR-Based Outlier Detection Complete ===\n",
      "  PCA analysis on cleaned data...\n",
      "\n",
      "=== Principal Component Analysis (PCA) ===\n",
      "Performing PCA on 4 variables with 693 samples\n",
      "Variables: ['PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
      "Optimal number of components for 95.0% explained variance: 2\n",
      "Kaiser Criterion recommended number of components: 1\n",
      "\n",
      "Number of components selected: 2\n",
      "Total explained variance: 0.9822\n",
      "\n",
      "Explained variance by component:\n",
      "  PC1: 0.8838 (88.38%)\n",
      "  PC2: 0.0984 (9.84%)\n",
      "\n",
      "Top loadings by component:\n",
      "  PC1:\n",
      "    Shape_Leng: 0.5072\n",
      "    AREA: 0.5008\n",
      "    PERIMETER: 0.4999\n",
      "  PC2:\n",
      "    Shape_Area: 0.5499\n",
      "    PERIMETER: -0.5318\n",
      "    Shape_Leng: -0.4572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scree plot saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/pca/pca_scree_plot.png\n",
      "PCA scatter plot saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/pca/pca_scatter_plot.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PCA Analysis Complete ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA loadings heatmap saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/pca/pca_loadings_heatmap.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating before/after comparison plots...\n",
      "Before/after comparison plots saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/cleaned_data/before_after_comparison\n",
      "\n",
      "\n",
      "==================================================\n",
      "OUTLIER DETECTION SUMMARY\n",
      "==================================================\n",
      "\n",
      "Z-score outliers (threshold=3.0):\n",
      "  Total: 22 rows (2.84% of data)\n",
      "  By column:\n",
      "    PERIMETER: 21 outliers\n",
      "    AREA: 13 outliers\n",
      "    Shape_Leng: 19 outliers\n",
      "    Shape_Area: 12 outliers\n",
      "\n",
      "IQR outliers (k=1.5):\n",
      "  Total: 428 values (55.15% of data cells)\n",
      "  By column:\n",
      "    AREA: 129 outliers\n",
      "    Shape_Area: 126 outliers\n",
      "    Shape_Leng: 87 outliers\n",
      "    PERIMETER: 86 outliers\n",
      "\n",
      "PCA-based outliers:\n",
      "  Total: 83 rows (10.70% of data)\n",
      "\n",
      "Method agreement summary:\n",
      "  Identified by 3 methods: 22 rows\n",
      "  Identified by 2 methods: 61 rows\n",
      "  Identified by 1 methods: 59 rows\n",
      "\n",
      "Cleaned data analysis:\n",
      "  Removal method: consensus\n",
      "  Outliers removed: 83 rows\n",
      "  Data reduction: 10.70%\n",
      "  Z-score outliers: 22 -> 29\n",
      "  IQR outliers: 428 -> 244\n",
      "\n",
      "HTML report saved to: /home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../outputs/DASC501/homework8/outlier_detection_20250602_040449/outlier_detection_report.html\n",
      "\n",
      "================================================================================\n",
      "OUTLIER DETECTION COMPLETE\n",
      "================================================================================\n",
      "2025-06-02 04:05:53,389 - INFO - Outlier detection process complete.\n",
      "2025-06-02 04:05:53,391 - INFO - Homework Part 1A script finished.\n",
      "2025-06-02 04:05:53,398 - INFO - Starting PySpark Military Bases Analysis\n",
      "2025-06-02 04:06:04,809 - INFO - Spark session initialized with robust Windows configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:04,815 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:65 - DatabaseManager initialized for DB: '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'\n",
      "2025-06-02 04:06:04,818 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:101 - Connecting to database: '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'...\n",
      "2025-06-02 04:06:04,822 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:105 - Database connection established successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:04,825 - INFO - Loading table 'military-bases' from '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:05,341 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:259 - SELECT query executed successfully. Found 776 records.\n",
      "2025-06-02 04:06:05,356 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:118 - Committing final changes (if any) and closing connection...\n",
      "2025-06-02 04:06:05,359 - DASC500.classes.DatabaseManager.DatabaseManager - INFO - DatabaseManager:122 - Database connection to '/home/jovyan/venv_notebook/src/dasc500/src/DASC500/../../data/DASC501/homework6/DataViz501.db' closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:05,362 - INFO - Raw data loaded: 776 rows, 14 columns\n",
      "2025-06-02 04:06:05,371 - INFO - Cleaned column names from ['Geo Point', 'Geo Shape', 'OBJECTID_1', 'OBJECTID', 'COMPONENT', 'Site Name', 'Joint Base', 'State Terr', 'COUNTRY', 'Oper Stat', 'PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area'] to ['Geo_Point', 'Geo_Shape', 'OBJECTID_1', 'OBJECTID', 'COMPONENT', 'Site_Name', 'Joint_Base', 'State_Terr', 'COUNTRY', 'Oper_Stat', 'PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
      "2025-06-02 04:06:05,432 - INFO - Cleaned 8 string columns: ['Geo_Point', 'Geo_Shape', 'COMPONENT', 'Site_Name', 'Joint_Base', 'State_Terr', 'COUNTRY', 'Oper_Stat']\n",
      "2025-06-02 04:06:05,436 - INFO - Cleaned geo column Geo_Point\n",
      "2025-06-02 04:06:05,443 - INFO - Cleaned geo column Geo_Shape\n",
      "2025-06-02 04:06:05,447 - INFO - Cleaned numeric column OBJECTID_1: int64 -> int64\n",
      "2025-06-02 04:06:05,451 - INFO - Cleaned numeric column OBJECTID: int64 -> int64\n",
      "2025-06-02 04:06:05,461 - INFO - Cleaned numeric column PERIMETER: float64 -> float64\n",
      "2025-06-02 04:06:05,471 - INFO - Cleaned numeric column AREA: float64 -> float64\n",
      "2025-06-02 04:06:05,480 - INFO - Cleaned numeric column Shape_Leng: float64 -> float64\n",
      "2025-06-02 04:06:05,490 - INFO - Cleaned numeric column Shape_Area: float64 -> float64\n",
      "2025-06-02 04:06:05,502 - INFO - Cleaned Joint_Base column\n",
      "2025-06-02 04:06:05,528 - INFO - Added Region column based on State_Terr\n",
      "2025-06-02 04:06:05,532 - INFO - Region distribution: {'South': 316, 'West': 236, 'Midwest': 113, 'Northeast': 79, 'Territory': 32}\n",
      "2025-06-02 04:06:05,568 - INFO - Final cleaned data: 776 rows, 15 columns\n",
      "2025-06-02 04:06:05,598 - INFO - Memory usage: 0.64 MB\n",
      "2025-06-02 04:06:05,601 - INFO - Null value counts by column:\n",
      "2025-06-02 04:06:11,535 - INFO - Converted pandas DataFrame to Spark DataFrame\n",
      "2025-06-02 04:06:11,537 - INFO - \n",
      "--- DataFrame Schema ---\n",
      "2025-06-02 04:06:11,577 - INFO - \n",
      "--- Sample Data ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Geo_Point: string (nullable = true)\n",
      " |-- Geo_Shape: string (nullable = true)\n",
      " |-- OBJECTID_1: integer (nullable = true)\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- COMPONENT: string (nullable = true)\n",
      " |-- Site_Name: string (nullable = true)\n",
      " |-- Joint_Base: string (nullable = true)\n",
      " |-- State_Terr: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- Oper_Stat: string (nullable = true)\n",
      " |-- PERIMETER: double (nullable = true)\n",
      " |-- AREA: double (nullable = true)\n",
      " |-- Shape_Leng: double (nullable = true)\n",
      " |-- Shape_Area: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:19,090 - INFO - === TASK A: SPARK SQL QUERIES ===\n",
      "2025-06-02 04:06:19,167 - INFO - Registered DataFrame as temporary view 'military_bases'\n",
      "2025-06-02 04:06:19,169 - INFO - Checking data structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+-----------+----------------------------+----------+----------+-------------+---------+------------+-------------+------------------+---------------------+------+\n",
      "|Geo_Point                    |Geo_Shape                                                                                                                                                                                                  |OBJECTID_1|OBJECTID|COMPONENT  |Site_Name                   |Joint_Base|State_Terr|COUNTRY      |Oper_Stat|PERIMETER   |AREA         |Shape_Leng        |Shape_Area           |Region|\n",
      "+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+-----------+----------------------------+----------+----------+-------------+---------+------------+-------------+------------------+---------------------+------+\n",
      "|31.2309993833, -85.6506347178|{\"coordinates\": [[[-85.65462565497243, 31.234178331412515], [-85.65280405303592, 31.2350202819558], [-85.65101108759404, 31.233844887000856], [-85.64624105229998, 31.231687174187147], [-85.64616034316...|26        |65      |Army Active|Allen Stagefield AL         |N/A       |Alabama   |United States|Active   |1.64138338  |0.17657484   |3170.633315936678 |627423.9946918904    |South |\n",
      "|31.8157331822, -85.6497984957|{\"coordinates\": [[[-85.65268851262239, 31.812802192409293], [-85.65271152656167, 31.816509553657813], [-85.65470472917532, 31.816518435825348], [-85.6546922210333, 31.818371903981546], [-85.6460822409...|33        |73      |Army Active|Louisville Stagefield AL    |N/A       |Alabama   |United States|Active   |1.7233805   |0.16235675   |3357.487241062    |584096.752713972     |South |\n",
      "|33.1594636742, -106.425696182|{\"coordinates\": [[[-106.27973443186896, 33.91098413413733], [-106.27967788921002, 33.90383877245734], [-106.26227558718553, 33.90372853850852], [-106.2623128196591, 33.896550230133286], [-106.26053414...|66        |261     |Army Active|White Sands Missile Range NM|N/A       |New Mexico|United States|Active   |332.13318926|3548.57016437|648984.1003717034 |1.3150788697079187E10|West  |\n",
      "|37.0130203962, -76.3043760544|{\"coordinates\": [[[[-76.29312038151345, 37.032907829086405], [-76.29312589177941, 37.03289158784458], [-76.29313136341779, 37.032875431936176], [-76.29313671917352, 37.032859535620396], [-76.293141878...|114       |899     |Army Active|Fort Monroe                 |N/A       |Virginia  |United States|Inactive |10.20968811 |0.87723297   |21033.880108846693|3570032.52737506     |South |\n",
      "|21.3866284869, -157.905641308|{\"coordinates\": [[[-157.8989279212737, 21.392721693901013], [-157.89843234229104, 21.39258164228016], [-157.89824108647548, 21.392617617712716], [-157.89820558595372, 21.39262429739379], [-157.8982062...|161       |1237    |MC Active  |MCB Camp Smith              |N/A       |Hawaii    |United States|Active   |2.9318851   |0.33124636   |5098.778435999114 |994400.4295987836    |West  |\n",
      "+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+--------+-----------+----------------------------+----------+----------+-------------+---------+------------+-------------+------------------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+\n",
      "|total_count|\n",
      "+-----------+\n",
      "|        776|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:22,118 - INFO - \n",
      "--- Query 1: Average area by component (simplified) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   COMPONENT|\n",
      "+------------+\n",
      "|  Army Guard|\n",
      "| Army Active|\n",
      "|   MC Active|\n",
      "|    AF Guard|\n",
      "|         WHS|\n",
      "|  MC Reserve|\n",
      "|Army Reserve|\n",
      "| Navy Active|\n",
      "|   AF Active|\n",
      "|  AF Reserve|\n",
      "+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:22,452 - INFO - Query 1 results:\n",
      "2025-06-02 04:06:25,205 - INFO - \n",
      "--- Query 2: Basic filtering and case statements ---\n",
      "2025-06-02 04:06:25,386 - INFO - Query 2 results:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+--------+--------+\n",
      "|   COMPONENT|base_count|avg_area|max_area|min_area|\n",
      "+------------+----------+--------+--------+--------+\n",
      "|   MC Active|        30|  127.67| 1190.22|    0.04|\n",
      "| Army Active|       134|  121.38| 3548.57|    0.02|\n",
      "|   AF Active|       137|   79.19| 4513.48|     0.0|\n",
      "|Army Reserve|         8|    45.6|  252.81|    0.12|\n",
      "| Navy Active|       208|   13.64|  957.47|     0.0|\n",
      "|  Army Guard|       177|   11.05|  228.79|    0.01|\n",
      "|  AF Reserve|        10|    1.98|    4.61|    0.08|\n",
      "|    AF Guard|        68|    1.21|   53.03|    0.02|\n",
      "|         WHS|         1|    0.34|    0.34|    0.34|\n",
      "|  MC Reserve|         3|    0.13|    0.18|    0.07|\n",
      "+------------+----------+--------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:26,543 - INFO - \n",
      "=== TASK B: PYSPARK DATAFRAME OPERATIONS ===\n",
      "2025-06-02 04:06:26,544 - INFO - \n",
      "--- B1: Filtering and Transformation ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+-------------+\n",
      "|           Site_Name|  COMPONENT| area_numeric|size_category|\n",
      "+--------------------+-----------+-------------+-------------+\n",
      "|Nellis Air Force ...|  AF Active|4513.48391165|   Very Large|\n",
      "|White Sands Missi...|Army Active|3548.57016437|   Very Large|\n",
      "|          Fort Bliss|Army Active|1742.55128279|   Very Large|\n",
      "|The Barry M Goldw...|  AF Active|1640.94082567|   Very Large|\n",
      "| Yuma Proving Ground|Army Active|1307.98611024|   Very Large|\n",
      "|Dugway Proving Gr...|Army Active| 1250.7702621|   Very Large|\n",
      "|    Twentynine Palms|  MC Active|1190.21534002|   Very Large|\n",
      "|  NTC and Fort Irwin|Army Active|1180.20496212|   Very Large|\n",
      "|Barry Goldwater R...|  MC Active|1082.62380838|   Very Large|\n",
      "|     Fort Wainwright|Army Active|1034.02242143|   Very Large|\n",
      "|     NAWS China Lake|Navy Active| 957.46719228|   Very Large|\n",
      "|Utah Test and Tra...|  AF Active|  900.7713739|   Very Large|\n",
      "| Randsburg Wash Area|Navy Active| 826.15256661|   Very Large|\n",
      "|Choc Mt Air Gnry Rng|  MC Active| 716.16304686|   Very Large|\n",
      "|Eglin AFB (Eglin ...|  AF Active| 703.11834606|   Very Large|\n",
      "|        UTTR - North|  AF Active| 575.80925514|   Very Large|\n",
      "|Camp Frank D Merrill|Army Active| 538.79293712|   Very Large|\n",
      "|Yakima Training C...|Army Active| 511.12106341|   Very Large|\n",
      "|         Edwards AFB|  AF Active| 481.54080981|   Very Large|\n",
      "|        Fort Stewart|Army Active| 436.71862465|   Very Large|\n",
      "+--------------------+-----------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:26,947 - INFO - Filtered and transformed data:\n",
      "2025-06-02 04:06:27,573 - INFO - \n",
      "--- B2: Grouping and Aggregation ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------+-------------+-------------+---------------+-------------+-------------+\n",
      "|Site_Name                   |COMPONENT  |AREA         |area_category|area_efficiency|is_joint_base|base_priority|\n",
      "+----------------------------+-----------+-------------+-------------+---------------+-------------+-------------+\n",
      "|White Sands Missile Range NM|Army Active|3548.57016437|Very Large   |10.684         |false        |2            |\n",
      "|Saylor Creek Air Force Range|AF Active  |171.09737601 |Large        |2.994          |false        |4            |\n",
      "|Fort Bliss                  |Army Active|1742.55128279|Very Large   |5.021          |false        |2            |\n",
      "|Pohakuloa Training Area     |Army Active|205.67476917 |Large        |2.309          |false        |4            |\n",
      "|UTTR - North                |AF Active  |575.80925514 |Very Large   |4.433          |false        |2            |\n",
      "+----------------------------+-----------+-------------+-------------+---------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:29,131 - INFO - Records with non-null Region: 776\n",
      "2025-06-02 04:06:30,108 - INFO - Grouped and aggregated data:\n",
      "2025-06-02 04:06:31,600 - INFO - \n",
      "--- B3: Join Operations ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+--------+-------------+-------------+----------+------------+----------+\n",
      "|COMPONENT   |Region   |base_count|avg_area|avg_perimeter|max_area     |min_area  |area_std_dev|total_area|\n",
      "+------------+---------+----------+--------+-------------+-------------+----------+------------+----------+\n",
      "|Army Active |West     |42        |247.3   |55.33        |3548.57016437|0.01811696|631.2       |10386.57  |\n",
      "|AF Active   |West     |53        |175.78  |40.68        |4513.48391165|0.00634495|664.14      |9316.25   |\n",
      "|Army Active |South    |60        |87.14   |39.78        |1742.55128279|0.01995549|246.02      |5228.14   |\n",
      "|MC Active   |West     |16        |208.85  |48.57        |1190.21534002|0.04127455|404.41      |3341.53   |\n",
      "|Navy Active |West     |73        |32.82   |16.08        |957.46719228 |0.01240696|146.64      |2396.09   |\n",
      "|AF Active   |South    |60        |23.45   |22.0         |703.11834606 |0.01747948|92.65       |1407.25   |\n",
      "|Army Guard  |South    |65        |10.75   |12.53        |216.30440833 |0.02044719|31.35       |698.46    |\n",
      "|Army Guard  |West     |37        |16.74   |15.03        |224.04205621 |0.00577393|42.89       |619.53    |\n",
      "|Army Guard  |Midwest  |46        |11.46   |19.5         |228.79220565 |0.01012664|36.68       |527.24    |\n",
      "|MC Active   |South    |14        |34.9    |59.89        |128.6658504  |0.15763888|46.5        |488.67    |\n",
      "|Army Active |Midwest  |19        |21.64   |19.47        |158.94663865 |0.10441299|43.41       |411.2     |\n",
      "|Army Reserve|West     |2         |128.28  |62.19        |252.81147345 |3.74509133|176.12      |256.56    |\n",
      "|Navy Active |South    |92        |2.73    |9.42         |22.11571068  |0.01514858|3.8         |251.03    |\n",
      "|Army Active |Northeast|13        |18.43   |16.33        |170.14390483 |0.01577849|46.59       |239.65    |\n",
      "|Army Reserve|Midwest  |3         |33.02   |26.68        |93.26753841  |0.19706563|52.25       |99.05     |\n",
      "|Navy Active |Midwest  |6         |16.35   |14.57        |96.85457877  |0.04823925|39.44       |98.08     |\n",
      "|Army Guard  |Northeast|26        |3.42    |6.79         |32.11642959  |0.0092156 |8.43        |88.85     |\n",
      "|AF Active   |Northeast|9         |7.92    |12.3         |48.42194574  |0.02040583|15.66       |71.31     |\n",
      "|AF Guard    |Midwest  |23        |2.88    |4.91         |53.03493845  |0.0238947 |11.0        |66.13     |\n",
      "|Navy Active |Territory|25        |2.19    |5.26         |24.08388864  |0.00252842|5.4         |54.86     |\n",
      "+------------+---------+----------+--------+-------------+-------------+----------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:32,319 - INFO - Unique regions in data: ['South', 'Midwest', 'West', 'Northeast', 'Territory']\n",
      "2025-06-02 04:06:32,402 - INFO - Regional information dataset:\n",
      "2025-06-02 04:06:33,030 - INFO - Joined data with calculated metrics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+-----------------+\n",
      "|   Region|   Coast_Type|Region_Base_Count|Region_Population|\n",
      "+---------+-------------+-----------------+-----------------+\n",
      "|    South|Atlantic/Gulf|                8|          1200000|\n",
      "|  Midwest|  Great Lakes|                2|           180000|\n",
      "|     West|      Pacific|                5|           850000|\n",
      "|Northeast|     Atlantic|                1|            95000|\n",
      "|Territory|      Unknown|                1|           100000|\n",
      "+---------+-------------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:34,037 - INFO - \n",
      "=== SUMMARY STATISTICS ===\n",
      "2025-06-02 04:06:34,039 - INFO - \n",
      "--- Basic Dataset Statistics ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------+------+-------------+------------+-----------------+-------------------------+------------------+\n",
      "|Site_Name                       |COMPONENT  |Region|Coast_Type   |AREA        |Region_Population|population_per_base_ratio|base_density_score|\n",
      "+--------------------------------+-----------+------+-------------+------------+-----------------+-------------------------+------------------+\n",
      "|NG MTA Clarks Hill Reservation  |Army Guard |South |Atlantic/Gulf|1.35364902  |1200000          |150000.0                 |1.128             |\n",
      "|NG Hammond Airport              |Army Guard |South |Atlantic/Gulf|0.15859909  |1200000          |150000.0                 |0.1322            |\n",
      "|Allegany Ballistics Lab         |Navy Active|South |Atlantic/Gulf|2.37061099  |1200000          |150000.0                 |1.9755            |\n",
      "|Camp Frank D Merrill            |Army Active|South |Atlantic/Gulf|538.79293712|1200000          |150000.0                 |448.9941          |\n",
      "|Longhorn AAP                    |Army Active|South |Atlantic/Gulf|2.27178501  |1200000          |150000.0                 |1.8932            |\n",
      "|Milan AAP                       |Army Active|South |Atlantic/Gulf|34.89137292 |1200000          |150000.0                 |29.0761           |\n",
      "|Goldberg Stagefield AL          |Army Active|South |Atlantic/Gulf|0.1611586   |1200000          |150000.0                 |0.1343            |\n",
      "|NSF Indian Head                 |Navy Active|South |Atlantic/Gulf|3.17933201  |1200000          |150000.0                 |2.6494            |\n",
      "|Naval Air Station Patuxent River|Navy Active|South |Atlantic/Gulf|9.70071567  |1200000          |150000.0                 |8.0839            |\n",
      "|NOLF Holley                     |Navy Active|South |Atlantic/Gulf|1.08474269  |1200000          |150000.0                 |0.904             |\n",
      "+--------------------------------+-----------+------+-------------+------------+-----------------+-------------------------+------------------+\n",
      "\n",
      "Total number of military bases: 776\n",
      "+------------+-----+\n",
      "|   COMPONENT|count|\n",
      "+------------+-----+\n",
      "| Navy Active|  208|\n",
      "|  Army Guard|  177|\n",
      "|   AF Active|  137|\n",
      "| Army Active|  134|\n",
      "|    AF Guard|   68|\n",
      "|   MC Active|   30|\n",
      "|  AF Reserve|   10|\n",
      "|Army Reserve|    8|\n",
      "|  MC Reserve|    3|\n",
      "|         WHS|    1|\n",
      "+------------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|Oper_Stat|count|\n",
      "+---------+-----+\n",
      "| Inactive|   23|\n",
      "|   Active|  753|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:35,997 - INFO - \n",
      "--- Numerical Column Statistics ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   Region|count|\n",
      "+---------+-----+\n",
      "|    South|  316|\n",
      "|  Midwest|  113|\n",
      "|     West|  236|\n",
      "|Northeast|   79|\n",
      "|Territory|   32|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:36,776 - INFO - \n",
      "=== ANALYSIS COMPLETE ===\n",
      "2025-06-02 04:06:36,778 - INFO - All tasks completed successfully!\n",
      "2025-06-02 04:06:36,864 - INFO - Final joined dataset cached for further use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|summary|             AREA|         PERIMETER|        Shape_Leng|          Shape_Area|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "|  count|              776|               776|               776|                 776|\n",
      "|   mean|46.65591324493562| 20.69916309059279|43431.845860767746|2.0089724651228058E8|\n",
      "| stddev|257.9263397712899|43.847761406611504| 92818.59297053916|1.1280222807082593E9|\n",
      "|    min|       0.00252842|        0.20921927| 340.0538925291862|   6966.990350806802|\n",
      "|    max|    4513.48391165|      489.09331858| 1017173.782289898|1.851155055961774...|\n",
      "+-------+-----------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 04:06:37,467 - INFO - Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from DASC500.classes.DatabaseManager import DatabaseManager\n",
    "from DASC500.stats.outlier_detection import run_outlier_detection\n",
    "from DASC500.utilities.get_top_level_module import get_top_level_module_path\n",
    "\n",
    "# --- Configuration for the main script ---\n",
    "FOLDER = os.path.join(get_top_level_module_path(), '../..')\n",
    "DB_PATH = os.path.join(FOLDER, \"data/DASC501/homework6/DataViz501.db\")\n",
    "TABLE_NAME = \"military-bases\" # Standardized table name\n",
    "COLUMNS_TO_ANALYZE = ['PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
    "OUTLIER_METHODS = ['iqr', 'zscore', 'pca']\n",
    "# Output directory for all reports and plots\n",
    "# The run_outlier_detection script will create a timestamped sub-folder within this\n",
    "MAIN_OUTPUT_DIR = os.path.join(FOLDER, \"outputs/DASC501/homework8\")\n",
    "\n",
    "\n",
    "# --- Set up basic logging for this script ---\n",
    "script_logger = logging.getLogger(__name__)\n",
    "script_logger.setLevel(logging.INFO)\n",
    "if not script_logger.hasHandlers():\n",
    "    # Console Handler for script's own logs\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    script_logger.addHandler(console_handler)\n",
    "\n",
    "\n",
    "def part1():\n",
    "    script_logger.info(\"Starting Part 1A: Outlier Detection an.d Reporting.\")\n",
    "\n",
    "    # --- 1. Create Dummy Database (or ensure your DB is available) ---\n",
    "    # For this example, we create a dummy DB. In a real scenario, DataViz501.db would exist.\n",
    "\n",
    "    # --- 2. Load Data using DatabaseManager ---\n",
    "    db_manager = DatabaseManager(db_path=DB_PATH)\n",
    "    df_military_bases = None\n",
    "    try:\n",
    "        with db_manager: # Connects and closes automatically\n",
    "            script_logger.info(f\"Attempting to load table '{TABLE_NAME}' from '{DB_PATH}'.\")\n",
    "            df_military_bases = db_manager.execute_select_query(f'SELECT * FROM `{TABLE_NAME}`')\n",
    "\n",
    "        if df_military_bases is None or df_military_bases.empty:\n",
    "            script_logger.error(f\"Failed to load data from table '{TABLE_NAME}' or table is empty.\")\n",
    "            return\n",
    "        script_logger.info(f\"Successfully loaded {len(df_military_bases)} rows from table '{TABLE_NAME}'.\")\n",
    "        script_logger.info(f\"DataFrame columns: {df_military_bases.columns.tolist()}\")\n",
    "\n",
    "        # Drop ID column if it was loaded, as it's not for outlier analysis\n",
    "        if 'ID' in df_military_bases.columns:\n",
    "            df_military_bases = df_military_bases.drop(columns=['ID'])\n",
    "            script_logger.info(\"Dropped 'ID' column from DataFrame.\")\n",
    "\n",
    "        # Verify that the necessary columns for analysis are present\n",
    "        missing_cols = [col for col in COLUMNS_TO_ANALYZE if col not in df_military_bases.columns]\n",
    "        if missing_cols:\n",
    "            script_logger.error(f\"The following columns required for analysis are missing from the loaded table: {missing_cols}\")\n",
    "            script_logger.error(f\"Available columns: {df_military_bases.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        script_logger.info(f\"Proceeding with columns for analysis: {COLUMNS_TO_ANALYZE}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"An error occurred during database interaction: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 3. Run Outlier Detection ---\n",
    "    if df_military_bases is not None and not df_military_bases.empty:\n",
    "        script_logger.info(\"Running comprehensive outlier detection...\")\n",
    "        try:\n",
    "            # The run_outlier_detection function will create its own timestamped subdirectory\n",
    "            # within MAIN_OUTPUT_DIR.\n",
    "            results = run_outlier_detection(\n",
    "                df=df_military_bases,\n",
    "                columns=COLUMNS_TO_ANALYZE,\n",
    "                methods=OUTLIER_METHODS,\n",
    "                output_dir=MAIN_OUTPUT_DIR, # Pass the main output directory\n",
    "                create_plots=True,       # Will save plots to files\n",
    "                show_plots=False,        # No interactive plot showing\n",
    "                save_csv=True,\n",
    "                include_descriptive_stats=True,\n",
    "                replot_without_outliers=True\n",
    "            )\n",
    "            script_logger.info(\"Outlier detection process complete.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            script_logger.error(f\"An error occurred during outlier detection: {e}\", exc_info=True)\n",
    "    else:\n",
    "        script_logger.warning(\"DataFrame is empty. Skipping outlier detection.\")\n",
    "\n",
    "    script_logger.info(\"Homework Part 1A script finished.\")\n",
    "\n",
    "# ------------ PART 2 ------------\n",
    "import os\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def initialize_spark_robust():\n",
    "    \"\"\"Initialize Spark session with robust Windows configuration\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MilitaryBasesAnalysis\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.host\", \"localhost\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "        .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.local.dir\", temp_dir) \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "        .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "        .config(\"spark.network.timeout\", \"600s\") \\\n",
    "        .config(\"spark.sql.execution.pyspark.udf.faulthandler.enabled\", \"true\") \\\n",
    "        .config(\"spark.python.worker.faulthandler.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .getOrCreate()  # Use only 1 core to avoid worker issues\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")  # Reduce logging noise\n",
    "    script_logger.info(\"Spark session initialized with robust Windows configuration\")\n",
    "    return spark\n",
    "\n",
    "\n",
    "def load_data_from_database():\n",
    "    \"\"\"Load military bases data from SQLite database with comprehensive cleanup\"\"\"\n",
    "    db_manager = DatabaseManager(db_path=DB_PATH)\n",
    "    df_pandas = None\n",
    "    \n",
    "    try:\n",
    "        with db_manager:\n",
    "            script_logger.info(f\"Loading table '{TABLE_NAME}' from '{DB_PATH}'\")\n",
    "            df_pandas = db_manager.execute_select_query(f'SELECT * FROM `{TABLE_NAME}`')\n",
    "        \n",
    "        if df_pandas is None or df_pandas.empty:\n",
    "            script_logger.error(f\"Failed to load data from table '{TABLE_NAME}' or table is empty\")\n",
    "            return None\n",
    "        \n",
    "        script_logger.info(f\"Raw data loaded: {len(df_pandas)} rows, {len(df_pandas.columns)} columns\")\n",
    "        \n",
    "        # COMPREHENSIVE DATA CLEANUP\n",
    "        \n",
    "        # 1. Clean column names - replace spaces with underscores and normalize\n",
    "        original_columns = df_pandas.columns.tolist()\n",
    "        df_pandas.columns = df_pandas.columns.str.replace(' ', '_', regex=False)\n",
    "        df_pandas.columns = df_pandas.columns.str.replace('-', '_', regex=False)\n",
    "        df_pandas.columns = df_pandas.columns.str.strip()\n",
    "        script_logger.info(f\"Cleaned column names from {original_columns} to {df_pandas.columns.tolist()}\")\n",
    "        \n",
    "        # 2. Clean ALL string columns - strip whitespace and handle empty strings\n",
    "        string_columns = df_pandas.select_dtypes(include=['object']).columns\n",
    "        for col in string_columns:\n",
    "            if col in df_pandas.columns:\n",
    "                # Convert to string first, then strip whitespace\n",
    "                df_pandas[col] = df_pandas[col].astype(str).str.strip()\n",
    "                # Replace 'nan', 'None', empty strings with actual NaN\n",
    "                df_pandas[col] = df_pandas[col].replace(['nan', 'None', '', '  ', 'null', 'NULL'], np.nan)\n",
    "                # For remaining strings, limit length to prevent serialization issues\n",
    "                df_pandas[col] = df_pandas[col].apply(lambda x: x[:500] if isinstance(x, str) and len(x) > 500 else x)\n",
    "                \n",
    "        script_logger.info(f\"Cleaned {len(string_columns)} string columns: {string_columns.tolist()}\")\n",
    "        \n",
    "        # 3. Handle specific problematic columns (Geo columns often cause issues)\n",
    "        geo_columns = ['Geo_Point', 'Geo_Shape']\n",
    "        for col in geo_columns:\n",
    "            if col in df_pandas.columns:\n",
    "                # Convert complex geo data to simple string representation\n",
    "                df_pandas[col] = df_pandas[col].astype(str)\n",
    "                # Truncate extremely long geo strings that might cause serialization issues\n",
    "                df_pandas[col] = df_pandas[col].apply(lambda x: x[:200] + \"...\" if len(str(x)) > 200 else str(x))\n",
    "                script_logger.info(f\"Cleaned geo column {col}\")\n",
    "        \n",
    "        # 4. Clean and validate numeric columns\n",
    "        numeric_columns = ['OBJECTID_1', 'OBJECTID', 'PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']\n",
    "        for col in numeric_columns:\n",
    "            if col in df_pandas.columns:\n",
    "                # Convert to numeric, coercing errors to NaN\n",
    "                original_dtype = df_pandas[col].dtype\n",
    "                df_pandas[col] = pd.to_numeric(df_pandas[col], errors='coerce')\n",
    "                \n",
    "                # Handle infinite values\n",
    "                df_pandas[col] = df_pandas[col].replace([np.inf, -np.inf], np.nan)\n",
    "                \n",
    "                # Cap extremely large values that might cause issues\n",
    "                if col in ['AREA', 'PERIMETER', 'Shape_Area', 'Shape_Leng']:\n",
    "                    max_reasonable = df_pandas[col].quantile(0.99) * 10  # 10x the 99th percentile\n",
    "                    df_pandas[col] = df_pandas[col].clip(upper=max_reasonable)\n",
    "                \n",
    "                script_logger.info(f\"Cleaned numeric column {col}: {original_dtype} -> {df_pandas[col].dtype}\")\n",
    "        \n",
    "        # 5. Handle Joint_Base column specifically\n",
    "        if \"Joint_Base\" in df_pandas.columns:\n",
    "            df_pandas[\"Joint_Base\"] = df_pandas[\"Joint_Base\"].fillna(\"N/A\").astype(str)\n",
    "            df_pandas[\"Joint_Base\"] = df_pandas[\"Joint_Base\"].str.strip()\n",
    "            script_logger.info(\"Cleaned Joint_Base column\")\n",
    "        \n",
    "        # 6. Add Region column with robust state mapping\n",
    "        def assign_region(state):\n",
    "            \"\"\"Assign region based on state/territory with robust handling\"\"\"\n",
    "            if pd.isna(state) or state == 'nan' or state == '':\n",
    "                return \"Unknown\"\n",
    "            \n",
    "            state = str(state).strip()\n",
    "            \n",
    "            # Define regional mappings - handle both abbreviations and full names\n",
    "            west_mapping = {\n",
    "                'CA': 'West', 'California': 'West',\n",
    "                'OR': 'West', 'Oregon': 'West', \n",
    "                'WA': 'West', 'Washington': 'West',\n",
    "                'NV': 'West', 'Nevada': 'West',\n",
    "                'ID': 'West', 'Idaho': 'West',\n",
    "                'UT': 'West', 'Utah': 'West',\n",
    "                'AZ': 'West', 'Arizona': 'West',\n",
    "                'MT': 'West', 'Montana': 'West',\n",
    "                'WY': 'West', 'Wyoming': 'West',\n",
    "                'CO': 'West', 'Colorado': 'West',\n",
    "                'NM': 'West', 'New Mexico': 'West',\n",
    "                'AK': 'West', 'Alaska': 'West',\n",
    "                'HI': 'West', 'Hawaii': 'West'\n",
    "            }\n",
    "            \n",
    "            south_mapping = {\n",
    "                'TX': 'South', 'Texas': 'South',\n",
    "                'OK': 'South', 'Oklahoma': 'South',\n",
    "                'AR': 'South', 'Arkansas': 'South',\n",
    "                'LA': 'South', 'Louisiana': 'South',\n",
    "                'MS': 'South', 'Mississippi': 'South',\n",
    "                'AL': 'South', 'Alabama': 'South',\n",
    "                'TN': 'South', 'Tennessee': 'South',\n",
    "                'KY': 'South', 'Kentucky': 'South',\n",
    "                'WV': 'South', 'West Virginia': 'South',\n",
    "                'VA': 'South', 'Virginia': 'South',\n",
    "                'NC': 'South', 'North Carolina': 'South',\n",
    "                'SC': 'South', 'South Carolina': 'South',\n",
    "                'GA': 'South', 'Georgia': 'South',\n",
    "                'FL': 'South', 'Florida': 'South',\n",
    "                'DE': 'South', 'Delaware': 'South',\n",
    "                'MD': 'South', 'Maryland': 'South',\n",
    "                'DC': 'South', 'District of Columbia': 'South'\n",
    "            }\n",
    "            \n",
    "            midwest_mapping = {\n",
    "                'ND': 'Midwest', 'North Dakota': 'Midwest',\n",
    "                'SD': 'Midwest', 'South Dakota': 'Midwest',\n",
    "                'NE': 'Midwest', 'Nebraska': 'Midwest',\n",
    "                'KS': 'Midwest', 'Kansas': 'Midwest',\n",
    "                'MN': 'Midwest', 'Minnesota': 'Midwest',\n",
    "                'IA': 'Midwest', 'Iowa': 'Midwest',\n",
    "                'MO': 'Midwest', 'Missouri': 'Midwest',\n",
    "                'WI': 'Midwest', 'Wisconsin': 'Midwest',\n",
    "                'IL': 'Midwest', 'Illinois': 'Midwest',\n",
    "                'IN': 'Midwest', 'Indiana': 'Midwest',\n",
    "                'MI': 'Midwest', 'Michigan': 'Midwest',\n",
    "                'OH': 'Midwest', 'Ohio': 'Midwest'\n",
    "            }\n",
    "            \n",
    "            northeast_mapping = {\n",
    "                'ME': 'Northeast', 'Maine': 'Northeast',\n",
    "                'NH': 'Northeast', 'New Hampshire': 'Northeast',\n",
    "                'VT': 'Northeast', 'Vermont': 'Northeast',\n",
    "                'MA': 'Northeast', 'Massachusetts': 'Northeast',\n",
    "                'RI': 'Northeast', 'Rhode Island': 'Northeast',\n",
    "                'CT': 'Northeast', 'Connecticut': 'Northeast',\n",
    "                'NY': 'Northeast', 'New York': 'Northeast',\n",
    "                'NJ': 'Northeast', 'New Jersey': 'Northeast',\n",
    "                'PA': 'Northeast', 'Pennsylvania': 'Northeast'\n",
    "            }\n",
    "            \n",
    "            # Combine all mappings\n",
    "            all_mappings = {**west_mapping, **south_mapping, **midwest_mapping, **northeast_mapping}\n",
    "            \n",
    "            # Try exact match first\n",
    "            if state in all_mappings:\n",
    "                return all_mappings[state]\n",
    "            \n",
    "            # Try case-insensitive match\n",
    "            for key, value in all_mappings.items():\n",
    "                if state.lower() == key.lower():\n",
    "                    return value\n",
    "            \n",
    "            # Handle territories and other cases\n",
    "            territories = ['PR', 'Puerto Rico', 'GU', 'Guam', 'VI', 'US Virgin Islands', \n",
    "                          'AS', 'American Samoa', 'MP', 'Northern Mariana Islands']\n",
    "            if any(state.lower() == t.lower() for t in territories):\n",
    "                return \"Territory\"\n",
    "            \n",
    "            return \"Other\"\n",
    "        \n",
    "        # Apply region mapping - check for state column\n",
    "        state_column = None\n",
    "        possible_state_columns = ['State_Terr', 'State', 'STATE', 'state', 'Territory', 'TERRITORY']\n",
    "        for col in possible_state_columns:\n",
    "            if col in df_pandas.columns:\n",
    "                state_column = col\n",
    "                break\n",
    "        \n",
    "        if state_column:\n",
    "            df_pandas['Region'] = df_pandas[state_column].apply(assign_region)\n",
    "            script_logger.info(f\"Added Region column based on {state_column}\")\n",
    "            region_counts = df_pandas['Region'].value_counts()\n",
    "            script_logger.info(f\"Region distribution: {region_counts.to_dict()}\")\n",
    "        else:\n",
    "            df_pandas['Region'] = \"Unknown\"\n",
    "            script_logger.warning(\"No state column found, assigned 'Unknown' to all regions\")\n",
    "        \n",
    "        # 7. Final data validation and cleanup\n",
    "        # Remove any rows that are completely empty\n",
    "        df_pandas = df_pandas.dropna(how='all')\n",
    "        \n",
    "        # Ensure all object columns are properly stringified and not too long\n",
    "        for col in df_pandas.select_dtypes(include=['object']).columns:\n",
    "            df_pandas[col] = df_pandas[col].astype(str)\n",
    "            df_pandas[col] = df_pandas[col].apply(lambda x: x[:300] if len(str(x)) > 300 else x)\n",
    "        \n",
    "        # Log final data statistics\n",
    "        script_logger.info(f\"Final cleaned data: {len(df_pandas)} rows, {len(df_pandas.columns)} columns\")\n",
    "        script_logger.info(f\"Memory usage: {df_pandas.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        script_logger.info(f\"Null value counts by column:\")\n",
    "        null_counts = df_pandas.isnull().sum()\n",
    "        for col, count in null_counts.items():\n",
    "            if count > 0:\n",
    "                script_logger.info(f\"  {col}: {count} nulls ({count/len(df_pandas)*100:.1f}%)\")\n",
    "        \n",
    "        return df_pandas\n",
    "        \n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"Error loading data from database: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_spark_dataframe_robust(spark, df_pandas):\n",
    "    \"\"\"Create Spark DataFrame with robust schema and additional safeguards\"\"\"\n",
    "    script_logger.info(\"Converting pandas DataFrame to Spark DataFrame with robust schema\")\n",
    "    \n",
    "    try:\n",
    "        # Further cleanup before Spark conversion\n",
    "        script_logger.info(\"Performing final cleanup before Spark conversion...\")\n",
    "        \n",
    "        # 1. Handle any remaining problematic values\n",
    "        df_clean = df_pandas.copy()\n",
    "        \n",
    "        # 2. Replace any remaining problematic string values\n",
    "        string_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "        for col in string_cols:\n",
    "            # Replace any remaining problematic values\n",
    "            df_clean[col] = df_clean[col].replace(['inf', '-inf', 'infinity', '-infinity'], 'Unknown')\n",
    "            # Ensure no None strings\n",
    "            df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "        \n",
    "        # 3. Handle numeric columns - ensure no inf values\n",
    "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "            # Fill remaining NaN with reasonable defaults\n",
    "            if col in ['AREA', 'PERIMETER', 'Shape_Area', 'Shape_Leng']:\n",
    "                df_clean[col] = df_clean[col].fillna(0.0)\n",
    "            else:\n",
    "                df_clean[col] = df_clean[col].fillna(0)\n",
    "        \n",
    "        # 4. Create explicit schema to avoid inference issues\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "        \n",
    "        schema_fields = []\n",
    "        \n",
    "        # Define schema based on actual columns present\n",
    "        for col in df_clean.columns:\n",
    "            if col in ['OBJECTID_1', 'OBJECTID']:\n",
    "                schema_fields.append(StructField(col, IntegerType(), True))\n",
    "            elif col in ['PERIMETER', 'AREA', 'Shape_Leng', 'Shape_Area']:\n",
    "                schema_fields.append(StructField(col, DoubleType(), True))\n",
    "            else:\n",
    "                schema_fields.append(StructField(col, StringType(), True))\n",
    "        \n",
    "        schema = StructType(schema_fields)\n",
    "        \n",
    "        script_logger.info(f\"Created schema with {len(schema_fields)} fields\")\n",
    "        \n",
    "        # 5. Convert data types to match schema\n",
    "        for field in schema.fields:\n",
    "            col_name = field.name\n",
    "            if col_name in df_clean.columns:\n",
    "                if isinstance(field.dataType, IntegerType):\n",
    "                    df_clean[col_name] = df_clean[col_name].astype('Int64')  # Nullable integer\n",
    "                elif isinstance(field.dataType, DoubleType):\n",
    "                    df_clean[col_name] = df_clean[col_name].astype('float64')\n",
    "                else:  # StringType\n",
    "                    df_clean[col_name] = df_clean[col_name].astype('str')\n",
    "        \n",
    "        # 6. Sample the data for testing if it's very large\n",
    "        if len(df_clean) > 10000:\n",
    "            script_logger.warning(f\"Large dataset detected ({len(df_clean)} rows). Consider sampling for testing.\")\n",
    "        \n",
    "        # 7. Create Spark DataFrame with explicit schema\n",
    "        df_spark = spark.createDataFrame(df_clean, schema=schema)\n",
    "        \n",
    "        # 8. Immediately cache and force evaluation to catch any issues early\n",
    "        df_spark = df_spark.cache()\n",
    "        count = df_spark.count()  # Force evaluation\n",
    "        script_logger.info(f\"Successfully created Spark DataFrame with {count} rows\")\n",
    "        \n",
    "        # 9. Repartition for better performance (optional)\n",
    "        optimal_partitions = max(1, min(8, count // 1000))  # Roughly 1000 rows per partition, max 8 partitions\n",
    "        if optimal_partitions > 1:\n",
    "            df_spark = df_spark.repartition(optimal_partitions)\n",
    "            script_logger.info(f\"Repartitioned DataFrame to {optimal_partitions} partitions\")\n",
    "        \n",
    "        return df_spark\n",
    "        \n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"Error creating Spark DataFrame: {e}\")\n",
    "        script_logger.error(f\"DataFrame shape: {df_pandas.shape}\")\n",
    "        script_logger.error(f\"DataFrame dtypes: {df_pandas.dtypes}\")\n",
    "        raise\n",
    "\n",
    "def task_a_spark_sql_queries(spark, df_spark):\n",
    "    \"\"\"Task A: Perform Spark SQL queries with error handling\"\"\"\n",
    "    script_logger.info(\"=== TASK A: SPARK SQL QUERIES ===\")\n",
    "    \n",
    "    # Register DataFrame as temporary view\n",
    "    df_spark.createOrReplaceTempView(\"military_bases\")\n",
    "    script_logger.info(\"Registered DataFrame as temporary view 'military_bases'\")\n",
    "    \n",
    "    try:\n",
    "        # First, let's see what data we have\n",
    "        script_logger.info(\"Checking data structure...\")\n",
    "        spark.sql(\"SELECT COUNT(*) as total_count FROM military_bases\").show()\n",
    "        spark.sql(\"SELECT DISTINCT COMPONENT FROM military_bases LIMIT 10\").show()\n",
    "        \n",
    "        # Query 1: Simplified aggregation to avoid worker crashes\n",
    "        script_logger.info(\"\\n--- Query 1: Average area by component (simplified) ---\")\n",
    "        query1 = \"\"\"\n",
    "        SELECT \n",
    "            COMPONENT,\n",
    "            COUNT(*) as base_count,\n",
    "            ROUND(AVG(CAST(AREA as DOUBLE)), 2) as avg_area,\n",
    "            ROUND(MAX(CAST(AREA as DOUBLE)), 2) as max_area,\n",
    "            ROUND(MIN(CAST(AREA as DOUBLE)), 2) as min_area\n",
    "        FROM (\n",
    "            SELECT * FROM military_bases \n",
    "            WHERE AREA IS NOT NULL \n",
    "            AND COMPONENT IS NOT NULL \n",
    "            AND CAST(AREA AS DOUBLE) IS NOT NULL\n",
    "        )\n",
    "        WHERE AREA IS NOT NULL AND COMPONENT IS NOT NULL\n",
    "        GROUP BY COMPONENT\n",
    "        ORDER BY avg_area DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        result1 = spark.sql(query1)\n",
    "        script_logger.info(\"Query 1 results:\")\n",
    "        result1.collect()\n",
    "        result1.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"Error in Query 1: {e}\")\n",
    "        result1 = None\n",
    "    \n",
    "    try:\n",
    "        # Query 2: Even simpler query to avoid complexity\n",
    "        script_logger.info(\"\\n--- Query 2: Basic filtering and case statements ---\")\n",
    "        query2 = \"\"\"\n",
    "        SELECT \n",
    "            Site_Name,\n",
    "            COMPONENT,\n",
    "            CAST(AREA as DOUBLE) as area_numeric,\n",
    "            CASE \n",
    "                WHEN CAST(AREA as DOUBLE) > 250 THEN 'Very Large'\n",
    "                WHEN CAST(AREA as DOUBLE) > 150 THEN 'Large'\n",
    "                WHEN CAST(AREA as DOUBLE) > 100 THEN 'Medium'\n",
    "                ELSE 'Small'\n",
    "            END as size_category\n",
    "        FROM military_bases \n",
    "        WHERE AREA IS NOT NULL \n",
    "            AND CAST(AREA as DOUBLE) > 100 \n",
    "            AND Oper_Stat = 'Active'\n",
    "        ORDER BY CAST(AREA as DOUBLE) DESC\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        result2 = spark.sql(query2)\n",
    "        script_logger.info(\"Query 2 results:\")\n",
    "        result2.collect()\n",
    "        result2.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"Error in Query 2: {e}\")\n",
    "        result2 = None\n",
    "    \n",
    "    return result1, result2\n",
    "\n",
    "def task_b_dataframe_operations(spark, df_spark):\n",
    "    \"\"\"Task B: Use PySpark DataFrame functions\"\"\"\n",
    "    script_logger.info(\"\\n=== TASK B: PYSPARK DATAFRAME OPERATIONS ===\")\n",
    "    \n",
    "    try:\n",
    "        # B1: Filtering and transformation on columns\n",
    "        script_logger.info(\"\\n--- B1: Filtering and Transformation ---\")\n",
    "        \n",
    "        # Filter active bases with area > 100 and add transformed columns\n",
    "        filtered_df = df_spark.filter(\n",
    "            (col(\"Oper_Stat\") == \"Active\") & \n",
    "            (col(\"AREA\") > 100)\n",
    "        ).withColumn(\n",
    "            \"area_category\", \n",
    "            when(col(\"AREA\") > 250, \"Very Large\")\n",
    "            .when(col(\"AREA\") > 150, \"Large\") \n",
    "            .when(col(\"AREA\") > 100, \"Medium\")\n",
    "            .otherwise(\"Small\")\n",
    "        ).withColumn(\n",
    "            \"area_efficiency\",\n",
    "            round(col(\"AREA\") / col(\"PERIMETER\"), 3)\n",
    "        ).withColumn(\n",
    "            \"is_joint_base\",\n",
    "            col(\"COMPONENT\") == \"Joint\"\n",
    "        ).withColumn(\n",
    "            \"base_priority\",\n",
    "            when(col(\"COMPONENT\") == \"Joint\", 1)\n",
    "            .when(col(\"area_category\") == \"Very Large\", 2)\n",
    "            .when(col(\"COMPONENT\").isin([\"Army\", \"Navy\", \"Air Force\"]), 3)\n",
    "            .otherwise(4)\n",
    "        )\n",
    "        \n",
    "        script_logger.info(\"Filtered and transformed data:\")\n",
    "        try:\n",
    "            filtered_df.select(\"Site_Name\", \"COMPONENT\", \"AREA\", \"area_category\", \n",
    "                              \"area_efficiency\", \"is_joint_base\", \"base_priority\").show(5, truncate=False)\n",
    "        except Exception as e:\n",
    "            script_logger.error(f\"Error showing filtered data: {e}\")\n",
    "            # Try showing just basic info\n",
    "            script_logger.info(f\"Filtered dataset has {filtered_df.count()} rows\")\n",
    "        \n",
    "        # B2: Grouping and aggregation with safer approach\n",
    "        script_logger.info(\"\\n--- B2: Grouping and Aggregation ---\")\n",
    "        \n",
    "        try:\n",
    "            # Check if Region column exists and has non-null values\n",
    "            region_count = df_spark.filter(col(\"Region\").isNotNull()).count()\n",
    "            script_logger.info(f\"Records with non-null Region: {region_count}\")\n",
    "            \n",
    "            if region_count > 0:\n",
    "                aggregated_df = df_spark.groupBy(\"COMPONENT\", \"Region\").agg(\n",
    "                    count(\"*\").alias(\"base_count\"),\n",
    "                    round(avg(\"AREA\"), 2).alias(\"avg_area\"),\n",
    "                    round(avg(\"PERIMETER\"), 2).alias(\"avg_perimeter\"),\n",
    "                    max(\"AREA\").alias(\"max_area\"),\n",
    "                    min(\"AREA\").alias(\"min_area\"),\n",
    "                    round(stddev(\"AREA\"), 2).alias(\"area_std_dev\"),\n",
    "                    round(sum(\"AREA\"), 2).alias(\"total_area\")\n",
    "                ).orderBy(desc(\"total_area\"))\n",
    "            else:\n",
    "                # Fallback: Group by COMPONENT only\n",
    "                script_logger.warning(\"No valid Region data found, grouping by COMPONENT only\")\n",
    "                aggregated_df = df_spark.groupBy(\"COMPONENT\").agg(\n",
    "                    count(\"*\").alias(\"base_count\"),\n",
    "                    round(avg(\"AREA\"), 2).alias(\"avg_area\"),\n",
    "                    round(avg(\"PERIMETER\"), 2).alias(\"avg_perimeter\"),\n",
    "                    max(\"AREA\").alias(\"max_area\"),\n",
    "                    min(\"AREA\").alias(\"min_area\"),\n",
    "                    round(stddev(\"AREA\"), 2).alias(\"area_std_dev\"),\n",
    "                    round(sum(\"AREA\"), 2).alias(\"total_area\")\n",
    "                ).orderBy(desc(\"total_area\"))\n",
    "            \n",
    "            script_logger.info(\"Grouped and aggregated data:\")\n",
    "            # Use limit to avoid potential memory issues\n",
    "            aggregated_df.show(truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            script_logger.error(f\"Error in aggregation: {e}\")\n",
    "            # Create a simple fallback aggregation\n",
    "            aggregated_df = df_spark.groupBy(\"COMPONENT\").count().orderBy(desc(\"count\"))\n",
    "            script_logger.info(\"Fallback aggregation - count by component:\")\n",
    "            aggregated_df.show()\n",
    "        \n",
    "        # B3: Join operations with safer approach\n",
    "        script_logger.info(\"\\n--- B3: Join Operations ---\")\n",
    "        \n",
    "        try:\n",
    "            # Create a simpler regional dataset that matches what we actually have\n",
    "            unique_regions = [row['Region'] for row in df_spark.select(\"Region\").distinct().collect()]\n",
    "            script_logger.info(f\"Unique regions in data: {unique_regions}\")\n",
    "            \n",
    "            # Create regional data that matches our actual regions\n",
    "            regional_data = []\n",
    "            for region in unique_regions:\n",
    "                if region == \"West\":\n",
    "                    regional_data.append((region, \"Pacific\", 5, 850000))\n",
    "                elif region == \"South\":\n",
    "                    regional_data.append((region, \"Atlantic/Gulf\", 8, 1200000))\n",
    "                elif region == \"Midwest\":\n",
    "                    regional_data.append((region, \"Great Lakes\", 2, 180000))\n",
    "                elif region == \"Northeast\":\n",
    "                    regional_data.append((region, \"Atlantic\", 1, 95000))\n",
    "                else:\n",
    "                    regional_data.append((region, \"Unknown\", 1, 100000))\n",
    "            \n",
    "            regional_schema = StructType([\n",
    "                StructField(\"Region\", StringType(), True),\n",
    "                StructField(\"Coast_Type\", StringType(), True),\n",
    "                StructField(\"Region_Base_Count\", IntegerType(), True),\n",
    "                StructField(\"Region_Population\", IntegerType(), True)\n",
    "            ])\n",
    "            \n",
    "            regional_df = spark.createDataFrame(regional_data, regional_schema)\n",
    "            \n",
    "            script_logger.info(\"Regional information dataset:\")\n",
    "            regional_df.show()\n",
    "            \n",
    "            # Perform inner join\n",
    "            joined_df = df_spark.join(regional_df, \"Region\", \"inner\")\n",
    "            \n",
    "            # Add calculated columns after join\n",
    "            final_df = joined_df.withColumn(\n",
    "                \"population_per_base_ratio\",\n",
    "                round(col(\"Region_Population\") / col(\"Region_Base_Count\"), 0)\n",
    "            ).withColumn(\n",
    "                \"base_density_score\",  \n",
    "                round(col(\"AREA\") / col(\"Region_Population\") * 1000000, 4)\n",
    "            )\n",
    "            \n",
    "            script_logger.info(\"Joined data with calculated metrics:\")\n",
    "            final_df.select(\"Site_Name\", \"COMPONENT\", \"Region\", \"Coast_Type\", \n",
    "                           \"AREA\", \"Region_Population\", \"population_per_base_ratio\", \n",
    "                           \"base_density_score\").limit(10).show(truncate=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            script_logger.error(f\"Error in join operations: {e}\")\n",
    "            # Return the original dataframe as fallback\n",
    "            final_df = df_spark\n",
    "        \n",
    "        return filtered_df, aggregated_df, final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"Error in task_b_dataframe_operations: {e}\")\n",
    "        # Return the original dataframe for all outputs as fallback\n",
    "        return df_spark, df_spark, df_spark\n",
    "\n",
    "def generate_summary_statistics(df_spark):\n",
    "    \"\"\"Generate comprehensive summary statistics\"\"\"\n",
    "    script_logger.info(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    script_logger.info(\"\\n--- Basic Dataset Statistics ---\")\n",
    "    print(f\"Total number of military bases: {df_spark.count()}\")\n",
    "    \n",
    "    df_spark.groupBy(\"COMPONENT\").count().orderBy(desc(\"count\")).show()\n",
    "    df_spark.groupBy(\"Oper_Stat\").count().show()\n",
    "    df_spark.groupBy(\"Region\").count().show()\n",
    "    \n",
    "    # Numerical statistics\n",
    "    script_logger.info(\"\\n--- Numerical Column Statistics ---\")\n",
    "    numerical_stats = df_spark.select(\"AREA\", \"PERIMETER\", \"Shape_Leng\", \"Shape_Area\").describe()\n",
    "    numerical_stats.show()\n",
    "\n",
    "\n",
    "def part2():\n",
    "    \"\"\"Main function to execute all tasks\"\"\"\n",
    "    script_logger.info(\"Starting PySpark Military Bases Analysis\")\n",
    "    \n",
    "    # Initialize Spark\n",
    "    spark = initialize_spark_robust()\n",
    "    \n",
    "    try:\n",
    "        # Try to load data from database first\n",
    "        df_pandas = load_data_from_database()\n",
    "        \n",
    "        # If database loading fails, use sample data\n",
    "        if df_pandas is None:\n",
    "            script_logger.info(\"Error loading data from database. Stopping execution.\")\n",
    "            raise ValueError(\"Failed to load data from database. Stopping execution.\")\n",
    "        \n",
    "        # Convert to Spark DataFrame\n",
    "        from pyspark.sql.types import StringType, StructType, IntegerType, DoubleType, StructField\n",
    "\n",
    "        schema = StructType([\n",
    "            StructField(\"Geo_Point\", StringType(), True),\n",
    "            StructField(\"Geo_Shape\", StringType(), True),\n",
    "            StructField(\"OBJECTID_1\", IntegerType(), True),\n",
    "            StructField(\"OBJECTID\", IntegerType(), True),\n",
    "            StructField(\"COMPONENT\", StringType(), True),\n",
    "            StructField(\"Site_Name\", StringType(), True),\n",
    "            StructField(\"Joint_Base\", StringType(), True),\n",
    "            StructField(\"State_Terr\", StringType(), True),\n",
    "            StructField(\"COUNTRY\", StringType(), True),\n",
    "            StructField(\"Oper_Stat\", StringType(), True),\n",
    "            StructField(\"PERIMETER\", DoubleType(), True),\n",
    "            StructField(\"AREA\", DoubleType(), True),\n",
    "            StructField(\"Shape_Leng\", DoubleType(), True),\n",
    "            StructField(\"Shape_Area\", DoubleType(), True),\n",
    "            StructField(\"Region\", StringType(), True),\n",
    "        ])\n",
    "\n",
    "        df_spark = spark.createDataFrame(df_pandas, schema=schema)\n",
    "        script_logger.info(\"Converted pandas DataFrame to Spark DataFrame\")\n",
    "        \n",
    "        # Show schema and sample data\n",
    "        script_logger.info(\"\\n--- DataFrame Schema ---\")\n",
    "        df_spark.printSchema()\n",
    "        \n",
    "        script_logger.info(\"\\n--- Sample Data ---\")\n",
    "        df_spark.show(5, truncate=False)\n",
    "        \n",
    "        # Execute Task A: Spark SQL queries\n",
    "        result1, result2 = task_a_spark_sql_queries(spark, df_spark)\n",
    "        \n",
    "        # Execute Task B: DataFrame operations\n",
    "        filtered_df, aggregated_df, joined_df = task_b_dataframe_operations(spark, df_spark)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        generate_summary_statistics(df_spark)\n",
    "        \n",
    "        script_logger.info(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "        script_logger.info(\"All tasks completed successfully!\")\n",
    "        \n",
    "        # Cache important results for potential further analysis\n",
    "        joined_df.cache()\n",
    "        script_logger.info(\"Final joined dataset cached for further use\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        script_logger.error(f\"An error occurred during analysis: {e}\", exc_info=True)\n",
    "        \n",
    "    finally:\n",
    "        # Clean up Spark session  \n",
    "        spark.stop()\n",
    "        script_logger.info(\"Spark session stopped\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    part1()\n",
    "    part2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e1de6-30d2-44e9-9316-d523a88cd618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
